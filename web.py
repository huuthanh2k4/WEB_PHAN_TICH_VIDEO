import os
import re
import time
import tempfile
import shutil
import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import whisper
import joblib
from transformers.pipelines import pipeline
from yt_dlp import YoutubeDL
from NPL.tien_xu_ly import TienXuLy
import gdown
import warnings

# --- Suppress non-critical Whisper / HF Hub warnings ---
warnings.filterwarnings(
    "ignore",
    message="FP16 is not supported on CPU; using FP32 instead"
)
warnings.filterwarnings(
    "ignore",
    message="resume_download is deprecated and will be removed"
)

# --- Th√™m ffmpeg_static v√†o PATH ƒë·ªÉ Whisper t√¨m ƒë∆∞·ª£c ffmpeg ---
ffmpeg_dir = os.path.join(os.getcwd(), "ffmpeg_static")
ffmpeg_path = os.path.join(ffmpeg_dir, "ffmpeg")
ffprobe_path = os.path.join(ffmpeg_dir, "ffprobe")
if os.path.isfile(ffmpeg_path) and os.path.isfile(ffprobe_path):
    try:
        os.chmod(ffmpeg_path, 0o755)
        os.chmod(ffprobe_path, 0o755)
    except Exception:
        pass
    os.environ["PATH"] = ffmpeg_dir + os.pathsep + os.environ.get("PATH", "")
    os.environ["FFMPEG_BINARY"] = ffmpeg_path


st.set_page_config(page_title="Subtitle & Emotion Analyzer", layout="wide")
st.title("üé¨ Ph√¢n t√≠ch ph·ª• ƒë·ªÅ & c·∫£m x√∫c t·ª´ video")

# --- Utility: download any video URL to MP4 via yt-dlp ---
def download_video(url: str, out_dir: str = "temp_video") -> str:
    os.makedirs(out_dir, exist_ok=True)
    ydl_opts = {
        "format": "bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]",
        "outtmpl": os.path.join(out_dir, "%(id)s.%(ext)s"),
        "merge_output_format": "mp4",
        "quiet": True,
    }
    with YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(url, download=True)
        return ydl.prepare_filename(info)

def download_from_drive(url: str, out_dir: str = "temp_video") -> str:
    """
    Nh·∫≠n URL share c·ªßa Google Drive, tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n t·ªõi file ƒë√£ t·∫£i v·ªÅ.
    """
    # extract file id
    m = re.search(r"/d/([a-zA-Z0-9_-]+)", url)
    if not m:
        st.error("URL Google Drive kh√¥ng ƒë√∫ng ƒë·ªãnh d·∫°ng '/d/<file_id>/'")
        st.stop()
    file_id = m.group(1)
    # x√¢y direct link
    download_url = f"https://drive.google.com/uc?id={file_id}"
    os.makedirs(out_dir, exist_ok=True)
    out_path = os.path.join(out_dir, file_id)  # t·ª± ƒë·∫∑t t√™n theo id
    # gdown t·ª± th√™m ƒëu√¥i file extension
    gdown.download(download_url, out_path, quiet=False)
    # n·∫øu gdown kh√¥ng t·ª± nh·∫≠n extension, b·∫°n c√≥ th·ªÉ √©p th√™m:
    # out_path = gdown.download(download_url, out_path + ".mp3", quiet=False)
    return out_path
    

# --- Cache heavy resources to speed up reruns ---
@st.cache_resource(show_spinner=False)
def load_whisper_model():
    return whisper.load_model("base")

@st.cache_resource(show_spinner=False)
def load_translator(src_lang: str):
    return pipeline("translation", model=f"Helsinki-NLP/opus-mt-{src_lang}-en")

@st.cache_resource(show_spinner=False)
def load_tfidf_model():
    return joblib.load("TF-IDF6.sav")

@st.cache_resource(show_spinner=False)
def load_svc_model():
    return joblib.load("SVC6.sav")

@st.cache_resource(show_spinner=False)
def load_svc_max():
    return joblib.load("SVC7.sav")

@st.cache_resource(show_spinner=False)
def load_tfidf_model1():
    return joblib.load("TF-IDF7.sav")

# Load models & processor once
whisper_model = load_whisper_model()
processor     = TienXuLy()
tfidf_model6    = load_tfidf_model()
tfidf_model7    = load_tfidf_model1()
svc_model      = load_svc_model()
svc_max       =  load_svc_max()

# --- Sidebar: choose input method ---
st.sidebar.header("1. Ch·ªçn ngu·ªìn video")
mode = st.sidebar.radio("Ch·ªçn:", ("T·∫£i l√™n file", "Nh·∫≠p URL"))


st.sidebar.header("Ch·ªçn model ƒë·ªÉ x·ª≠ l√Ω")
chon_model = st.sidebar.radio(
    "Model:",
    ("ph√¢n lo·∫°i 1 c√≥ 7 lo·∫°i", "ph√¢n lo·∫°i 2 c√≥ 6 lo·∫°i")
)

if chon_model == "ph√¢n lo·∫°i 1 c√≥ 7 lo·∫°i" :
    chon_model = svc_max
    tfidf = tfidf_model7
elif chon_model == "ph√¢n lo·∫°i 2 c√≥ 6 lo·∫°i" :
    chon_model = svc_model
    tfidf = tfidf_model6

video_path = None
if mode == "T·∫£i l√™n file":
    uploaded = st.sidebar.file_uploader(
        "Ch·ªçn video ho·∫∑c audio", 
        type=["mp4","mov","avi","mp3"]
    )
    if uploaded:
        # Ch·ªçn suffix d·ª±a v√†o lo·∫°i file
        if uploaded.name.lower().endswith(".mp3"):
            suffix = ".mp3"
        else:
            _, ext = os.path.splitext(uploaded.name)
            suffix = ext or ".mp4"
        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
            tmp.write(uploaded.read())
            video_path = tmp.name
elif mode == "Nh·∫≠p URL":
    url = st.sidebar.text_input("Nh·∫≠p link video (YouTube/TikTok/Vimeo/... ho·∫∑c Google Drive):")
    if url:
        try:
            with st.spinner("‚è≥ ƒêang t·∫£i v·ªÅ..."):
                if "drive.google.com" in url:
                    video_path = download_from_drive(url)
                else:
                    video_path = download_video(url)
            st.sidebar.success(f"‚úîÔ∏è ƒê√£ t·∫£i v·ªÅ: {os.path.basename(video_path)}")
        except Exception as e:
            st.sidebar.error(f"‚ùå T·∫£i video th·∫•t b·∫°i: {e}")
            st.stop()

if not video_path:
    st.sidebar.warning("Vui l√≤ng cung c·∫•p video ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
    st.stop()
else:
    st.sidebar.success(f"‚úîÔ∏è ƒê√£ ch·ªçn video: {os.path.basename(video_path)}")

# --- Auto-reset khi ng∆∞·ªùi d√πng ch·ªçn video m·ªõi ---
if "last_video" not in st.session_state:
    st.session_state.last_video = None

if video_path and st.session_state.last_video != video_path:
    # X√≥a m·ªçi state c≈© tr·ª´ last_video
    for key in list(st.session_state.keys()):
        if key != "last_video":
            del st.session_state[key]
    st.session_state.last_video = video_path

# --- Start processing ---
t0 = time.perf_counter()

st.header("2. Transcription")
with st.spinner("‚è≥ ƒêang ch·∫°y Whisper transcription..."):
    transcription = whisper_model.transcribe(video_path)
segments = transcription["segments"]
lang = transcription["language"]
st.write(f"üî§ Ph√°t hi·ªán ng√¥n ng·ªØ: **{lang}**")

# 3) Optional translation to English
if lang != "en":
    st.header("3. Translation")
    with st.spinner("‚è≥ ƒêang d·ªãch sang ti·∫øng Anh..."):
        translator = load_translator(lang)
        for seg in segments:
            try:
                # D·ªãch tr∆∞·ªùng 'text' c·ªßa segment
                seg["text"] = translator(seg["text"])[0]["translation_text"]
            except Exception:
                seg["text"] = "[L·ªói d·ªãch thu·∫≠t]"

# 4) Build DataFrame & preprocess text
st.header("4. Build DataFrame & Preprocessing")
vi_translator = pipeline("translation", model="Helsinki-NLP/opus-mt-en-vi")

records = []
for seg in segments:
    txt = seg["text"].strip()

    # D·ªãch sang ti·∫øng Vi·ªát
    try:
        viet = vi_translator(txt)[0]["translation_text"]
    except Exception:
        viet = "[L·ªói d·ªãch sang ti·∫øng Vi·ªát]"

    records.append({
        "start":                   seg["start"],
        "end":                     seg["end"],
        "ph·ª• ƒë·ªÅ":                  txt,
        "Ti·∫øng Vi·ªát":              viet,
        "x·ª≠ l√Ω ph·ª• ƒë·ªÅ cho model":  processor.prepare_data(txt)
    })

df = pd.DataFrame(records)
st.dataframe(df, use_container_width=True)


st.header("5. Emotion Classification")
with st.spinner("‚è≥ M√£ ho√° TF-IDF v√† d·ª± ƒëo√°n c·∫£m x√∫c..."):
    X = tfidf.transform(df["x·ª≠ l√Ω ph·ª• ƒë·ªÅ cho model"])
    preds = chon_model.predict(X)
label_map = {
    "0": "bu·ªìn",    "1": "vui",    "2": "y√™u",
    "3": "gi·∫≠n d·ªØ", "4": "s·ª£ h√£i","5": "ng·∫°c nhi√™n",
    "6": "ko ch·ª©a c·∫£m x√∫c"
}
df["C·∫£m x√∫c"] = pd.Series(preds.astype(str)).map(label_map)
st.dataframe(df[["start", "end", "ph·ª• ƒë·ªÅ", "C·∫£m x√∫c"]], use_container_width=True)

t1 = time.perf_counter()
elapsed = t1 - t0
if elapsed < 60:
    st.success(f"‚úÖ Ho√†n th√†nh trong **{elapsed:.2f} gi√¢y**")
else:
    st.success(f"‚úÖ Ho√†n th√†nh trong **{elapsed/60:.2f} ph√∫t**")

st.header("6. Th·ªëng k√™ C·∫£m x√∫c")
counts = df["C·∫£m x√∫c"].value_counts()
fig1, ax1 = plt.subplots()
explode = [0.1 if v/counts.sum() < 0.1 else 0 for v in counts.values]
ax1.pie(counts.values, labels=counts.index, explode=explode,
        autopct="%1.1f%%", startangle=140,
        wedgeprops={"linewidth":1, "edgecolor":"white"})
ax1.axis("equal")
st.pyplot(fig1)

fig2, ax2 = plt.subplots(figsize=(10, 6))
ax2.bar(counts.index, counts.values)
ax2.set_xlabel("C·∫£m x√∫c")
ax2.set_ylabel("S·ªë l∆∞·ª£ng")
ax2.set_title("S·ªë l∆∞·ª£ng m·ªói c·∫£m x√∫c")
st.pyplot(fig2)

st.header("7. T·∫£i v·ªÅ k·∫øt qu·∫£")
csv_data = df.to_csv(index=False, encoding="utf-8-sig")
st.download_button("üì• T·∫£i CSV", data=csv_data,
                   file_name=f"{os.path.splitext(os.path.basename(video_path))[0]}_subtitles.csv",
                   mime="text/csv")

# --- N√∫t reset to√†n b·ªô ƒë·ªÉ ph√¢n t√≠ch video m·ªõi ---
if st.button("üîÑ Ph√¢n t√≠ch video m·ªõi", key="reset_btn"):
    for key in list(st.session_state.keys()):
        del st.session_state[key]
    if os.path.isdir("temp_video"):
        shutil.rmtree("temp_video", ignore_errors=True)
    st.experimental_rerun()

# --- Cleanup file t·∫°m n·∫øu l√† upload ---
if mode == "T·∫£i l√™n file" and video_path:
    os.remove(video_path)
elif mode == "Nh·∫≠p URL" and video_path:
    shutil.rmtree("temp_video", ignore_errors=True)
